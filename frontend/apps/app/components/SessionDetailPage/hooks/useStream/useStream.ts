'use client'

import {
  type BaseMessage,
  coerceMessageLikeToMessage,
  isHumanMessage,
} from '@langchain/core/messages'
import { SSE_EVENTS } from '@liam-hq/agent/client'
import { err, ok } from 'neverthrow'
import { useCallback, useEffect, useRef, useState } from 'react'
import { LG_INITIAL_MESSAGE_PREFIX } from '../../../../constants/storageKeys'
import { ERROR_MESSAGES } from '../../components/Chat/constants/chatConstants'
import { MessageTupleManager } from './MessageTupleManager'
import { parseSse } from './parseSse'

type ChatRequest = {
  userInput: string
  designSessionId: string
  isDeepModelingEnabled: boolean
}

/**
 * NOTE: Custom hook based on useStream from @langchain/langgraph-sdk
 * @see https://github.com/langchain-ai/langgraphjs/blob/3320793bffffa02682227644aefbee95dee330a2/libs/sdk/src/react/stream.tsx
 */
type Props = {
  designSessionId: string
  initialMessages: BaseMessage[]
}
export const useStream = ({ designSessionId, initialMessages }: Props) => {
  const [messages, setMessages] = useState<BaseMessage[]>(initialMessages)
  const messageManagerRef = useRef(new MessageTupleManager())

  const [isStreaming, setIsStreaming] = useState(false)
  const abortRef = useRef<AbortController | null>(null)

  const stop = useCallback(() => {
    abortRef.current?.abort()
    abortRef.current = null
  }, [])

  // biome-ignore lint/complexity/noExcessiveCognitiveComplexity: TODO: Refactor to reduce complexity
  const start = useCallback(async (params: ChatRequest) => {
    abortRef.current?.abort()
    abortRef.current = new AbortController()
    setIsStreaming(true)

    try {
      const res = await fetch('/api/chat/stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(params),
        signal: abortRef.current.signal,
      })

      if (!res.body) {
        return err({
          type: 'network',
          message: ERROR_MESSAGES.FETCH_FAILED,
          status: res.status,
        })
      }

      for await (const ev of parseSse(res.body)) {
        if (ev.event === SSE_EVENTS.END) {
          setIsStreaming(false)
          break
        }

        if (ev.event !== SSE_EVENTS.MESSAGES) continue

        const parsedData = JSON.parse(ev.data)
        const [serialized, metadata] = parsedData
        const messageId = messageManagerRef.current.add(serialized, metadata)
        if (!messageId) continue

        setMessages((prev) => {
          const newMessages = [...prev]
          const result = messageManagerRef.current.get(messageId, prev.length)
          if (!result?.chunk) return newMessages

          const { chunk, index } = result
          const message = coerceMessageLikeToMessage(chunk)

          if (index === undefined) {
            newMessages.push(message)
          } else {
            newMessages[index] = message
          }

          return newMessages
        })
      }

      setIsStreaming(false)
      return ok(undefined)
    } catch (error) {
      if (error instanceof Error && error.name === 'AbortError') {
        return err({
          type: 'abort',
          message: 'Request was aborted',
        })
      }
      return err({
        type: 'unknown',
        message: ERROR_MESSAGES.GENERAL,
      })
    }
  }, [])

  useEffect(() => {
    const key = `${LG_INITIAL_MESSAGE_PREFIX}:${designSessionId}`
    const stored = sessionStorage.getItem(key)
    if (!stored) return
    const msg = coerceMessageLikeToMessage(JSON.parse(stored))
    sessionStorage.removeItem(key)
    if (isHumanMessage(msg)) setMessages((prev) => [msg, ...prev])
  }, [designSessionId])

  return {
    messages,
    isStreaming,
    stop,
    start,
  }
}
